# Plastinka Sales Predictor API

A FastAPI application for predicting vinyl record sales using time series forecasting, with cloud-based computation via Yandex DataSphere.

## ðŸ“š Documentation

This API is a core component of the Plastinka Sales Predictor system. For a high-level overview of the entire project, including its ML module and infrastructure, please refer to the [main project README](../README.md).

This document focuses specifically on the API application, its usage, and internal architecture.

## Data Flow

1. **Data Upload**:
   - Upload Excel (.xlsx, .xls) or CSV files with stock and sales data
   - Automatic file format detection and encoding handling (UTF-8, Windows-1251, CP1252)
   - Automatic CSV separator detection (comma, semicolon)
   - Data is validated, processed, and stored in SQLite database
   - Records are mapped to multi-index structure for time series analysis

2. **Training & Prediction** (Combined process):
   - Data is prepared and uploaded directly to Yandex DataSphere project storage
   - Training job is submitted to Yandex DataSphere
   - TiDE model is trained with two-phase approach:
     1. First training with early stopping to determine optimal epochs
     2. Final training on full dataset with optimized parameters
   - Predictions are generated immediately after training
   - Both model and predictions are downloaded to the local server
   - Model file is stored locally and metadata in the database
   - Predictions are stored in the database for later reporting
   - No separate prediction API call is needed
   - DataSphere project storage automatically cleans up temporary files

3. **Reporting**:
   - Uses stored prediction data to generate HTML reports

## Directory Structure

```
deployment/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/                  # API routes and request/response models
â”‚   â”‚   â”œâ”€â”€ admin.py          # Admin endpoints (system info, cleanup)
â”‚   â”‚   â”œâ”€â”€ health.py         # Health check endpoints
â”‚   â”‚   â”œâ”€â”€ jobs.py           # Job management endpoints
â”‚   â”‚   â””â”€â”€ models_configs.py # Model parameter management
â”‚   â”œâ”€â”€ db/                   # Database schema, operations, and data retention logic
â”‚   â”‚   â”œâ”€â”€ database.py       # Core database operations
â”‚   â”‚   â”œâ”€â”€ schema.py         # Database table definitions
â”‚   â”‚   â”œâ”€â”€ data_retention.py # Automated data cleanup
â”‚   â”‚   â””â”€â”€ feature_storage.py # Feature engineering data storage
â”‚   â”œâ”€â”€ services/             # Business logic services
â”‚   â”‚   â”œâ”€â”€ auth.py           # Authentication service
â”‚   â”‚   â”œâ”€â”€ data_processor.py # Data processing and validation
â”‚   â”‚   â”œâ”€â”€ datasphere_service.py # Yandex DataSphere integration
â”‚   â”‚   â””â”€â”€ report_service.py # Report generation service
â”‚   â”œâ”€â”€ utils/                # Utility functions
â”‚   â”‚   â”œâ”€â”€ error_handling.py # Error handling and retry logic
â”‚   â”‚   â”œâ”€â”€ file_validation.py # File upload validation
â”‚   â”‚   â”œâ”€â”€ retry.py          # Retry mechanisms
â”‚   â”‚   â””â”€â”€ validation.py     # Data validation utilities
â”‚   â”œâ”€â”€ config.py             # Configuration management (Pydantic settings)
â”‚   â”œâ”€â”€ logger_config.py      # Centralized logging configuration
â”‚   â””â”€â”€ main.py               # FastAPI application setup and main entry point
â”œâ”€â”€ datasphere/
â”‚   â”œâ”€â”€ client.py             # Yandex DataSphere client for API interactions
â”‚   â””â”€â”€ prepare_datasets.py   # Scripts for preparing datasets for DataSphere jobs
â”œâ”€â”€ infrastructure/           # Terraform infrastructure as code
â”‚   â”œâ”€â”€ modules/              # Reusable Terraform modules
â”‚   â”‚   â”œâ”€â”€ datasphere_community/ # DataSphere community setup
â”‚   â”‚   â”œâ”€â”€ datasphere_project/   # DataSphere project configuration
â”‚   â”‚   â””â”€â”€ service_account/      # Service account and IAM setup
â”‚   â””â”€â”€ envs/
â”‚       â””â”€â”€ prod/             # Production environment configuration
â”œâ”€â”€ scripts/                  # Deployment and utility scripts
â”‚   â””â”€â”€ check_environment.py  # Script to validate environment setup
â””â”€â”€ run.py                    # Main runner script for the API application
```

**Data Storage Structure** (created at runtime based on `DATA_ROOT_DIR`):
```
~/.plastinka_sales_predictor/    # Default data root directory
â”œâ”€â”€ database/
â”‚   â””â”€â”€ plastinka.db            # SQLite database file
â”œâ”€â”€ models/                     # Storage for trained model artifacts (e.g., .onnx files)
â”œâ”€â”€ datasphere_input/           # Prepared datasets for DataSphere jobs
â”œâ”€â”€ datasphere_output/          # Downloaded results from DataSphere
â”œâ”€â”€ predictions/                # Saved prediction outputs
â”œâ”€â”€ reports/                    # Generated reports
â”œâ”€â”€ logs/                       # Application logs
```



## ðŸ“š Documentation

This API is a core component of the Plastinka Sales Predictor system. For a high-level overview of the entire project, including its ML module and infrastructure, please refer to the [main project README](../README.md).

This document focuses specifically on the API application, its usage, and internal architecture.

#

## Troubleshooting

For general troubleshooting steps, common issues, and logging information, please refer to the [main project README](../README.md).