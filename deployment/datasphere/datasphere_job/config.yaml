# Configuration for Yandex DataSphere Job
# This file defines the environment, inputs, outputs, and commands for the training and prediction pipeline.

# --- Job Metadata ---
name: plastinka-sales-prediction-job
desc: Trains a TiDE model, generates sales predictions, and saves the model and metrics.

# --- Compute Resources ---
# Define the resources and VM configuration for the job.
# Replace <instance-type> with an appropriate instance type from Yandex Cloud.
# Example: 'g1.1' for a standard CPU instance, or 'g2.8' for a GPU instance.
cloud_instance_types:
  - <instance-type> # e.g., g1.1

# Attach the project disk to access project files if needed. Recommended.
attach_project_disk: true

# --- Job Inputs ---
# These files will be uploaded and made available inside the job's environment in the /project/inputs/ directory.
# The .whl, .dill, and .json files must exist at the specified paths relative to the project root
# when you run `datasphere project job execute`.
inputs:
  # Core script and its requirements
  - deployment/datasphere/datasphere_job/train_and_predict.py
  - deployment/datasphere/datasphere_job/requirements.txt

  # --- TODO: Replace placeholders with your actual file paths ---
  # These files should be placed in the `deployment/datasphere/datasphere_job/` directory or you should provide a correct relative path.
  - deployment/datasphere/datasphere_job/params.json                 # Parameters for the training script.
  - deployment/datasphere/datasphere_job/train.dill                  # Preprocessed training dataset.
  - deployment/datasphere/datasphere_job/val.dill                    # Preprocessed validation dataset.
  - deployment/datasphere/datasphere_job/plastinka_sales_predictor-0.1.0-py3-none-any.whl # The packaged library.

# --- Job Outputs ---
# These files will be generated by the script in /project/outputs/ and collected by DataSphere.
outputs:
  - model.onnx
  - predictions.csv
  - metrics.json

# --- Environment Configuration ---
env:
  python:
    # Use a manual configuration for the Python environment.
    type: manual
    # Specify the Python version. Should match the one used to build the wheel.
    version: '3.8'
    # Specify the requirements file. It will be installed via pip from the inputs.
    requirements-file: /project/inputs/requirements.txt

# --- Execution Command ---
# The main command to execute in the job. It's a multi-step shell script.
cmd: |
  set -e
  echo "--- Installing custom wheel package ---"
  # Install the wheel file provided in the inputs. A wildcard is used to match any version.
  # --no-deps is used because other dependencies are handled by requirements.txt.
  # --force-reinstall ensures the correct version is installed.
  pip install --no-deps --force-reinstall /project/inputs/plastinka_sales_predictor-*.whl
  
  echo "--- Running training and prediction script ---"
  # Run the main Python script.
  # All inputs are located in /project/inputs/.
  # The output directory is set to /project/outputs/, where DataSphere will collect the results.
  python /project/inputs/train_and_predict.py \
    --input /project/inputs/params.json \
    --output '{"dir": "/project/outputs/", "model": "model.onnx", "prediction": "predictions.csv", "metrics": "metrics.json"}' 